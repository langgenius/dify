model: openai/gpt-4
label:
  en_US: gpt-4
model_type: llm
features:
  - multi-tool-call
  - agent-thought
  - stream-tool-call
model_properties:
  mode: chat
  context_size: 8192
parameter_rules:
  - name: temperature
    use_template: temperature
  - name: top_p
    use_template: top_p
  - name: top_k
    label:
      zh_Hans: 取样数量
      en_US: Top k
    type: int
    help:
      zh_Hans: 仅从每个后续标记的前 K 个选项中采样。
      en_US: Only sample from the top K options for each subsequent token.
    required: false
  - name: presence_penalty
    use_template: presence_penalty
  - name: frequency_penalty
    use_template: frequency_penalty
  - name: max_tokens
    use_template: max_tokens
    default: 512
    min: 1
    max: 8192
  - name: seed
    label:
      zh_Hans: 种子
      en_US: Seed
    type: int
    help:
      zh_Hans:
        如果指定，模型将尽最大努力进行确定性采样，使得重复的具有相同种子和参数的请求应该返回相同的结果。不能保证确定性，您应该参考 system_fingerprint
        响应参数来监视变化。
      en_US:
        If specified, model will make a best effort to sample deterministically,
        such that repeated requests with the same seed and parameters should return
        the same result. Determinism is not guaranteed, and you should refer to the
        system_fingerprint response parameter to monitor changes in the backend.
    required: false
  - name: response_format
    label:
      zh_Hans: 回复格式
      en_US: Response Format
    type: string
    help:
      zh_Hans: 指定模型必须输出的格式
      en_US: specifying the format that the model must output
    required: false
    options:
      - text
      - json_object
pricing:
  input: "30"
  output: "60"
  unit: "0.000001"
  currency: USD
