app:
  description: 'This chatflow contains 2 LLM, LLM 1 always speak English, LLM 2 always
    speak Chinese.


    2 LLMs run parallel, but LLM 2 will output before LLM 1, so we can see all LLM
    2 chunks, then LLM 1 chunks.


    All chunks should be send before Answer Node started.'
  icon: ðŸ¤–
  icon_background: '#FFEAD5'
  mode: advanced-chat
  name: test_parallel_streaming
  use_icon_as_answer_icon: false
dependencies:
- current_identifier: null
  type: marketplace
  value:
    marketplace_plugin_unique_identifier: langgenius/openai:0.0.30@1f5ecdef108418a467e54da2dcf5de2cf22b47632abc8633194ac9fb96317ede
kind: app
version: 0.3.1
workflow:
  conversation_variables: []
  environment_variables: []
  features:
    file_upload:
      allowed_file_extensions:
      - .JPG
      - .JPEG
      - .PNG
      - .GIF
      - .WEBP
      - .SVG
      allowed_file_types:
      - image
      allowed_file_upload_methods:
      - local_file
      - remote_url
      enabled: false
      fileUploadConfig:
        audio_file_size_limit: 50
        batch_count_limit: 5
        file_size_limit: 15
        image_file_size_limit: 10
        video_file_size_limit: 100
        workflow_file_upload_limit: 10
      image:
        enabled: false
        number_limits: 3
        transfer_methods:
        - local_file
        - remote_url
      number_limits: 3
    opening_statement: ''
    retriever_resource:
      enabled: true
    sensitive_word_avoidance:
      enabled: false
    speech_to_text:
      enabled: false
    suggested_questions: []
    suggested_questions_after_answer:
      enabled: false
    text_to_speech:
      enabled: false
      language: ''
      voice: ''
  graph:
    edges:
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: start
        targetType: llm
      id: 1754336720803-source-1754339718571-target
      source: '1754336720803'
      sourceHandle: source
      target: '1754339718571'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: start
        targetType: llm
      id: 1754336720803-source-1754339725656-target
      source: '1754336720803'
      sourceHandle: source
      target: '1754339725656'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: llm
        targetType: answer
      id: 1754339718571-source-answer-target
      source: '1754339718571'
      sourceHandle: source
      target: answer
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: llm
        targetType: answer
      id: 1754339725656-source-answer-target
      source: '1754339725656'
      sourceHandle: source
      target: answer
      targetHandle: target
      type: custom
      zIndex: 0
    nodes:
    - data:
        desc: ''
        selected: false
        title: Start
        type: start
        variables: []
      height: 54
      id: '1754336720803'
      position:
        x: 30
        y: 252.5
      positionAbsolute:
        x: 30
        y: 252.5
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        answer: '{{#1754339725656.text#}}{{#1754339718571.text#}}'
        desc: ''
        selected: true
        title: Answer
        type: answer
        variables: []
      height: 105
      id: answer
      position:
        x: 638
        y: 252.5
      positionAbsolute:
        x: 638
        y: 252.5
      selected: true
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        memory:
          query_prompt_template: '{{#sys.query#}}


            {{#sys.files#}}'
          role_prefix:
            assistant: ''
            user: ''
          window:
            enabled: false
            size: 50
        model:
          completion_params:
            temperature: 0.7
          mode: chat
          name: gpt-4o
          provider: langgenius/openai/openai
        prompt_template:
        - id: e8ef0664-d560-4017-85f2-9a40187d8a53
          role: system
          text: Always speak English.
        selected: false
        title: LLM 1
        type: llm
        variables: []
        vision:
          enabled: false
      height: 90
      id: '1754339718571'
      position:
        x: 334
        y: 252.5
      positionAbsolute:
        x: 334
        y: 252.5
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        memory:
          query_prompt_template: '{{#sys.query#}}


            {{#sys.files#}}'
          role_prefix:
            assistant: ''
            user: ''
          window:
            enabled: false
            size: 50
        model:
          completion_params:
            temperature: 0.7
          mode: chat
          name: gpt-4o
          provider: langgenius/openai/openai
        prompt_template:
        - id: 326169b2-0817-4bc2-83d6-baf5c9efd175
          role: system
          text: Always speak Chinese.
        selected: false
        title: LLM 2
        type: llm
        variables: []
        vision:
          enabled: false
      height: 90
      id: '1754339725656'
      position:
        x: 334
        y: 382.5
      positionAbsolute:
        x: 334
        y: 382.5
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    viewport:
      x: -108.49999999999994
      y: 229.5
      zoom: 0.7
