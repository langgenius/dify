#!/usr/bin/env python3
"""
Clickzetta Vector Database Integration Test Suite
æµ‹è¯•ç”¨ä¾‹è¦†ç›– Clickzetta å‘é‡æ•°æ®åº“çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
"""

import os
import sys
import time
import threading
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any
import numpy as np

# Add the API path to sys.path for imports
sys.path.insert(0, '/Users/liangmo/Documents/GitHub/dify/api')

from core.rag.datasource.vdb.clickzetta.clickzetta_vector import ClickzettaVector
from core.rag.models.document import Document

class ClickzettaTestSuite:
    """Clickzetta å‘é‡æ•°æ®åº“æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.vector_db = None
        self.test_results = []
        self.collection_name = "test_collection_" + str(int(time.time()))
        
    def setup(self):
        """æµ‹è¯•ç¯å¢ƒè®¾ç½®"""
        try:
            config = {
                'username': os.getenv('CLICKZETTA_USERNAME'),
                'password': os.getenv('CLICKZETTA_PASSWORD'),
                'instance': os.getenv('CLICKZETTA_INSTANCE'),
                'service': os.getenv('CLICKZETTA_SERVICE', 'uat-api.clickzetta.com'),
                'workspace': os.getenv('CLICKZETTA_WORKSPACE'),
                'vcluster': os.getenv('CLICKZETTA_VCLUSTER', 'default_ap'),
                'schema': os.getenv('CLICKZETTA_SCHEMA', 'dify')
            }
            
            # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
            required_vars = ['username', 'password', 'instance', 'workspace']
            missing_vars = [var for var in required_vars if not config[var]]
            if missing_vars:
                raise Exception(f"Missing required environment variables: {missing_vars}")
            
            self.vector_db = ClickzettaVector(
                collection_name=self.collection_name,
                config=config
            )
            
            print(f"âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®æˆåŠŸï¼Œä½¿ç”¨é›†åˆ: {self.collection_name}")
            return True
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥: {str(e)}")
            return False
    
    def cleanup(self):
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        try:
            if self.vector_db:
                self.vector_db.delete()
            print("âœ… æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æµ‹è¯•æ•°æ®æ—¶å‡ºé”™: {str(e)}")
    
    def generate_test_documents(self, count: int = 10) -> List[Document]:
        """ç”Ÿæˆæµ‹è¯•æ–‡æ¡£"""
        documents = []
        for i in range(count):
            doc = Document(
                page_content=f"è¿™æ˜¯æµ‹è¯•æ–‡æ¡£ {i+1}ï¼ŒåŒ…å«å…³äºäººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ çš„å†…å®¹ã€‚",
                metadata={
                    'doc_id': f'test_doc_{i+1}',
                    'source': f'test_source_{i+1}',
                    'category': 'test',
                    'index': i
                }
            )
            documents.append(doc)
        return documents
    
    def test_basic_operations(self):
        """æµ‹è¯•åŸºç¡€æ“ä½œï¼šåˆ›å»ºã€æ’å…¥ã€æŸ¥è¯¢ã€åˆ é™¤"""
        print("\nğŸ§ª æµ‹è¯•åŸºç¡€æ“ä½œ...")
        
        try:
            # 1. æµ‹è¯•æ–‡æ¡£æ’å…¥
            test_docs = self.generate_test_documents(5)
            embeddings = [np.random.rand(1536).tolist() for _ in range(5)]
            
            start_time = time.time()
            ids = self.vector_db.add_texts(
                texts=[doc.page_content for doc in test_docs],
                embeddings=embeddings,
                metadatas=[doc.metadata for doc in test_docs]
            )
            insert_time = time.time() - start_time
            
            assert len(ids) == 5, f"æœŸæœ›æ’å…¥5ä¸ªæ–‡æ¡£ï¼Œå®é™…æ’å…¥{len(ids)}ä¸ª"
            print(f"âœ… æ–‡æ¡£æ’å…¥æˆåŠŸï¼Œè€—æ—¶: {insert_time:.2f}ç§’")
            
            # 2. æµ‹è¯•ç›¸ä¼¼æ€§æœç´¢
            start_time = time.time()
            query_embedding = np.random.rand(1536).tolist()
            results = self.vector_db.similarity_search_by_vector(
                embedding=query_embedding,
                k=3
            )
            search_time = time.time() - start_time
            
            assert len(results) <= 3, f"æœŸæœ›æœ€å¤šè¿”å›3ä¸ªç»“æœï¼Œå®é™…è¿”å›{len(results)}ä¸ª"
            print(f"âœ… ç›¸ä¼¼æ€§æœç´¢æˆåŠŸï¼Œè¿”å›{len(results)}ä¸ªç»“æœï¼Œè€—æ—¶: {search_time:.2f}ç§’")
            
            # 3. æµ‹è¯•æ–‡æœ¬æœç´¢
            start_time = time.time()
            text_results = self.vector_db.similarity_search(
                query="äººå·¥æ™ºèƒ½",
                k=2
            )
            text_search_time = time.time() - start_time
            
            print(f"âœ… æ–‡æœ¬æœç´¢æˆåŠŸï¼Œè¿”å›{len(text_results)}ä¸ªç»“æœï¼Œè€—æ—¶: {text_search_time:.2f}ç§’")
            
            # 4. æµ‹è¯•æ–‡æ¡£åˆ é™¤
            if ids:
                start_time = time.time()
                self.vector_db.delete_by_ids([ids[0]])
                delete_time = time.time() - start_time
                print(f"âœ… æ–‡æ¡£åˆ é™¤æˆåŠŸï¼Œè€—æ—¶: {delete_time:.2f}ç§’")
            
            self.test_results.append({
                'test': 'basic_operations',
                'status': 'PASS',
                'metrics': {
                    'insert_time': insert_time,
                    'search_time': search_time,
                    'text_search_time': text_search_time,
                    'delete_time': delete_time
                }
            })
            
        except Exception as e:
            print(f"âŒ åŸºç¡€æ“ä½œæµ‹è¯•å¤±è´¥: {str(e)}")
            self.test_results.append({
                'test': 'basic_operations',
                'status': 'FAIL',
                'error': str(e)
            })
    
    def test_concurrent_operations(self):
        """æµ‹è¯•å¹¶å‘æ“ä½œå®‰å…¨æ€§"""
        print("\nğŸ§ª æµ‹è¯•å¹¶å‘æ“ä½œ...")
        
        try:
            def insert_batch(batch_id: int, batch_size: int = 5):
                """æ‰¹é‡æ’å…¥æ“ä½œ"""
                try:
                    docs = self.generate_test_documents(batch_size)
                    embeddings = [np.random.rand(1536).tolist() for _ in range(batch_size)]
                    
                    # ä¸ºæ¯ä¸ªæ‰¹æ¬¡æ·»åŠ å”¯ä¸€æ ‡è¯†
                    for i, doc in enumerate(docs):
                        doc.metadata['batch_id'] = batch_id
                        doc.metadata['doc_id'] = f'batch_{batch_id}_doc_{i}'
                    
                    ids = self.vector_db.add_texts(
                        texts=[doc.page_content for doc in docs],
                        embeddings=embeddings,
                        metadatas=[doc.metadata for doc in docs]
                    )
                    return f"Batch {batch_id}: æˆåŠŸæ’å…¥ {len(ids)} ä¸ªæ–‡æ¡£"
                except Exception as e:
                    return f"Batch {batch_id}: å¤±è´¥ - {str(e)}"
            
            # å¯åŠ¨å¤šä¸ªå¹¶å‘æ’å…¥ä»»åŠ¡
            start_time = time.time()
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = [executor.submit(insert_batch, i) for i in range(3)]
                results = [future.result() for future in futures]
            
            concurrent_time = time.time() - start_time
            
            # æ£€æŸ¥ç»“æœ
            success_count = sum(1 for result in results if "æˆåŠŸ" in result)
            print(f"âœ… å¹¶å‘æ“ä½œå®Œæˆï¼Œ{success_count}/3 ä¸ªæ‰¹æ¬¡æˆåŠŸï¼Œæ€»è€—æ—¶: {concurrent_time:.2f}ç§’")
            
            for result in results:
                print(f"  - {result}")
            
            self.test_results.append({
                'test': 'concurrent_operations',
                'status': 'PASS' if success_count >= 2 else 'PARTIAL',
                'metrics': {
                    'concurrent_time': concurrent_time,
                    'success_rate': success_count / 3
                }
            })
            
        except Exception as e:
            print(f"âŒ å¹¶å‘æ“ä½œæµ‹è¯•å¤±è´¥: {str(e)}")
            self.test_results.append({
                'test': 'concurrent_operations',
                'status': 'FAIL',
                'error': str(e)
            })
    
    def test_performance_benchmark(self):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\nğŸ§ª æµ‹è¯•æ€§èƒ½åŸºå‡†...")
        
        try:
            batch_sizes = [10, 50, 100]
            performance_results = {}
            
            for batch_size in batch_sizes:
                print(f"  æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
                
                # ç”Ÿæˆæµ‹è¯•æ•°æ®
                docs = self.generate_test_documents(batch_size)
                embeddings = [np.random.rand(1536).tolist() for _ in range(batch_size)]
                
                # æµ‹è¯•æ’å…¥æ€§èƒ½
                start_time = time.time()
                ids = self.vector_db.add_texts(
                    texts=[doc.page_content for doc in docs],
                    embeddings=embeddings,
                    metadatas=[doc.metadata for doc in docs]
                )
                insert_time = time.time() - start_time
                
                # æµ‹è¯•æœç´¢æ€§èƒ½
                query_embedding = np.random.rand(1536).tolist()
                start_time = time.time()
                results = self.vector_db.similarity_search_by_vector(
                    embedding=query_embedding,
                    k=10
                )
                search_time = time.time() - start_time
                
                performance_results[batch_size] = {
                    'insert_time': insert_time,
                    'insert_rate': batch_size / insert_time,
                    'search_time': search_time,
                    'results_count': len(results)
                }
                
                print(f"    æ’å…¥: {insert_time:.2f}ç§’ ({batch_size/insert_time:.1f} docs/sec)")
                print(f"    æœç´¢: {search_time:.2f}ç§’ (è¿”å›{len(results)}ä¸ªç»“æœ)")
            
            self.test_results.append({
                'test': 'performance_benchmark',
                'status': 'PASS',
                'metrics': performance_results
            })
            
        except Exception as e:
            print(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
            self.test_results.append({
                'test': 'performance_benchmark',
                'status': 'FAIL',
                'error': str(e)
            })
    
    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        print("\nğŸ§ª æµ‹è¯•é”™è¯¯å¤„ç†...")
        
        try:
            test_cases = []
            
            # 1. æµ‹è¯•æ— æ•ˆåµŒå…¥ç»´åº¦
            try:
                invalid_embedding = [1.0, 2.0, 3.0]  # é”™è¯¯çš„ç»´åº¦
                self.vector_db.add_texts(
                    texts=["æµ‹è¯•æ–‡æœ¬"],
                    embeddings=[invalid_embedding]
                )
                test_cases.append("invalid_embedding: FAIL - åº”è¯¥æŠ›å‡ºå¼‚å¸¸")
            except Exception:
                test_cases.append("invalid_embedding: PASS - æ­£ç¡®å¤„ç†æ— æ•ˆç»´åº¦")
            
            # 2. æµ‹è¯•ç©ºæ–‡æœ¬
            try:
                result = self.vector_db.add_texts(
                    texts=[""],
                    embeddings=[np.random.rand(1536).tolist()]
                )
                test_cases.append("empty_text: PASS - å¤„ç†ç©ºæ–‡æœ¬")
            except Exception as e:
                test_cases.append(f"empty_text: HANDLED - {str(e)[:50]}")
            
            # 3. æµ‹è¯•å¤§æ‰¹é‡æ•°æ®
            try:
                large_batch = self.generate_test_documents(1000)
                embeddings = [np.random.rand(1536).tolist() for _ in range(1000)]
                
                start_time = time.time()
                ids = self.vector_db.add_texts(
                    texts=[doc.page_content for doc in large_batch],
                    embeddings=embeddings,
                    metadatas=[doc.metadata for doc in large_batch]
                )
                large_batch_time = time.time() - start_time
                
                test_cases.append(f"large_batch: PASS - å¤„ç†1000ä¸ªæ–‡æ¡£ï¼Œè€—æ—¶{large_batch_time:.2f}ç§’")
            except Exception as e:
                test_cases.append(f"large_batch: HANDLED - {str(e)[:50]}")
            
            for case in test_cases:
                print(f"  - {case}")
            
            self.test_results.append({
                'test': 'error_handling',
                'status': 'PASS',
                'test_cases': test_cases
            })
            
        except Exception as e:
            print(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {str(e)}")
            self.test_results.append({
                'test': 'error_handling',
                'status': 'FAIL',
                'error': str(e)
            })
    
    def test_full_text_search(self):
        """æµ‹è¯•å…¨æ–‡æœç´¢åŠŸèƒ½"""
        print("\nğŸ§ª æµ‹è¯•å…¨æ–‡æœç´¢...")
        
        try:
            # æ’å…¥å¸¦æœ‰ç‰¹å®šå…³é”®è¯çš„æ–‡æ¡£
            search_docs = [
                Document(
                    page_content="Pythonæ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œäººå·¥æ™ºèƒ½é¢†åŸŸã€‚",
                    metadata={'category': 'programming', 'language': 'python'}
                ),
                Document(
                    page_content="æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥å¸®åŠ©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚",
                    metadata={'category': 'ai', 'topic': 'machine_learning'}
                ),
                Document(
                    page_content="å‘é‡æ•°æ®åº“æ˜¯å­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡æ•°æ®çš„ä¸“ç”¨æ•°æ®åº“ç³»ç»Ÿã€‚",
                    metadata={'category': 'database', 'type': 'vector'}
                )
            ]
            
            embeddings = [np.random.rand(1536).tolist() for _ in range(3)]
            
            # æ’å…¥æµ‹è¯•æ–‡æ¡£
            ids = self.vector_db.add_texts(
                texts=[doc.page_content for doc in search_docs],
                embeddings=embeddings,
                metadatas=[doc.metadata for doc in search_docs]
            )
            
            # æµ‹è¯•ä¸åŒçš„æœç´¢æŸ¥è¯¢
            search_queries = [
                ("Python", "programming"),
                ("æœºå™¨å­¦ä¹ ", "ai"),
                ("å‘é‡", "database"),
                ("æ•°æ®", "general")
            ]
            
            search_results = {}
            for query, expected_category in search_queries:
                results = self.vector_db.similarity_search(query=query, k=5)
                search_results[query] = {
                    'count': len(results),
                    'results': [r.metadata.get('category', 'unknown') for r in results if hasattr(r, 'metadata')]
                }
                print(f"  æŸ¥è¯¢ '{query}': è¿”å› {len(results)} ä¸ªç»“æœ")
            
            self.test_results.append({
                'test': 'full_text_search',
                'status': 'PASS',
                'search_results': search_results
            })
            
        except Exception as e:
            print(f"âŒ å…¨æ–‡æœç´¢æµ‹è¯•å¤±è´¥: {str(e)}")
            self.test_results.append({
                'test': 'full_text_search',
                'status': 'FAIL',
                'error': str(e)
            })
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š Clickzetta å‘é‡æ•°æ®åº“æµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result['status'] == 'PASS')
        failed_tests = sum(1 for result in self.test_results if result['status'] == 'FAIL')
        partial_tests = sum(1 for result in self.test_results if result['status'] == 'PARTIAL')
        
        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"é€šè¿‡: {passed_tests}")
        print(f"å¤±è´¥: {failed_tests}")
        print(f"éƒ¨åˆ†é€šè¿‡: {partial_tests}")
        print(f"æˆåŠŸç‡: {(passed_tests + partial_tests) / total_tests * 100:.1f}%")
        
        print(f"\nè¯¦ç»†ç»“æœ:")
        for result in self.test_results:
            status_emoji = {"PASS": "âœ…", "FAIL": "âŒ", "PARTIAL": "âš ï¸"}
            print(f"{status_emoji.get(result['status'], 'â“')} {result['test']}: {result['status']}")
            
            if 'metrics' in result:
                for key, value in result['metrics'].items():
                    if isinstance(value, dict):
                        print(f"    {key}:")
                        for k, v in value.items():
                            print(f"      {k}: {v}")
                    else:
                        print(f"    {key}: {value}")
            
            if 'error' in result:
                print(f"    é”™è¯¯: {result['error']}")
        
        return {
            'summary': {
                'total': total_tests,
                'passed': passed_tests,
                'failed': failed_tests,
                'partial': partial_tests,
                'success_rate': (passed_tests + partial_tests) / total_tests * 100
            },
            'details': self.test_results
        }
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ Clickzetta å‘é‡æ•°æ®åº“é›†æˆæµ‹è¯•")
        
        if not self.setup():
            return False
        
        try:
            self.test_basic_operations()
            self.test_concurrent_operations()
            self.test_performance_benchmark()
            self.test_error_handling()
            self.test_full_text_search()
            
        finally:
            self.cleanup()
        
        return self.generate_test_report()

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_env_vars = [
        'CLICKZETTA_USERNAME',
        'CLICKZETTA_PASSWORD', 
        'CLICKZETTA_INSTANCE',
        'CLICKZETTA_WORKSPACE'
    ]
    
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    if missing_vars:
        print(f"âŒ ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {missing_vars}")
        print("è¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡:")
        for var in required_env_vars:
            print(f"export {var}=your_value")
        return False
    
    # è¿è¡Œæµ‹è¯•å¥—ä»¶
    test_suite = ClickzettaTestSuite()
    report = test_suite.run_all_tests()
    
    if report:
        print(f"\nğŸ¯ æµ‹è¯•å®Œæˆï¼æˆåŠŸç‡: {report['summary']['success_rate']:.1f}%")
        return report['summary']['success_rate'] > 80
    
    return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)